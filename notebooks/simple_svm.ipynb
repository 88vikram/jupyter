{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from tadpole.models import SimpleSVM\n",
    "from tadpole.utils import test_train_split\n",
    "\n",
    "tadpoleD1D2File = Path(\"/Users/jvdzwaan/data/TADPOLE/data/TADPOLE_D1_D2.csv\")\n",
    "\n",
    "df = pd.read_csv(tadpoleD1D2File)\n",
    "\n",
    "# remove all rows without\n",
    "df = df[df[\"Ventricles\"].notnull()]\n",
    "df = df[df[\"DXCHANGE\"].notnull()]\n",
    "df = df[df[\"ADAS13\"].notnull()]\n",
    "\n",
    "train_df, test_df, ground_truth_df = test_train_split(df, 0.1)\n",
    "\n",
    "model = SimpleSVM()\n",
    "model.train(train_df)\n",
    "\n",
    "first_patient_id = test_df['RID'][0]\n",
    "prediction_orig = model.predict(test_df[test_df['RID'] == first_patient_id], datetime.datetime.now())\n",
    "truth_orig = ground_truth_df[ground_truth_df['RID'] == first_patient_id]\n",
    "\n",
    "# evaluate (prediction, truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CognitiveAssessmentDate = ExamDate (source: https://github.com/tadpole-share/TADPOLE-eval/blob/c8d4e241bc143b858d9b8237aab92417d3e871e2/evaluation/makeLeaderboardDataset.py#L159)\n",
    "* ScanDate = ExamDate (source: https://github.com/tadpole-share/TADPOLE-eval/blob/c8d4e241bc143b858d9b8237aab92417d3e871e2/evaluation/makeLeaderboardDataset.py#L162)\n",
    "* Diagnosis = DXCHANGE with mapping (source: https://github.com/tadpole-share/TADPOLE-eval/blob/c8d4e241bc143b858d9b8237aab92417d3e871e2/evaluation/makeLeaderboardDataset.py#L160 and line 169-170)\n",
    "* RID = Patient id (what is it used for? Why must it be set?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tadpole.transformations import get_diagnosis\n",
    "\n",
    "# make copies so we don't have to train all the time\n",
    "prediction = prediction_orig.copy()\n",
    "truth = truth_orig.copy()\n",
    "\n",
    "# The Forecast date has to be a string\n",
    "p = pd.DataFrame(prediction)\n",
    "p['Forecast Date'] = p['Forecast Date'].astype(str)\n",
    "print(p.dtypes)\n",
    "\n",
    "# Add expected columns\n",
    "truth['CognitiveAssessmentDate'] = truth['EXAMDATE']\n",
    "truth['ScanDate'] = truth['EXAMDATE']\n",
    "\n",
    "truth['Diagnosis'] = get_diagnosis(truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in ground_truth_df.columns:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outcomes_equal(result1, result2):\n",
    "    \"\"\"Method that checks whether two evaluation outcomes are equal.\n",
    "    \"\"\"\n",
    "    for i1, i2 in zip(result1, result2):\n",
    "        if isinstance(i1, pd.Series):\n",
    "            r = i1.equals(i2)\n",
    "        else:\n",
    "            r = i1 == i1 or np.isnan(i1) and np.isnan(i2)\n",
    "\n",
    "        if not r:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import evalOneSubmissionExtended, simpleEvalOneSubmissionExtended\n",
    "\n",
    "p = pd.DataFrame(prediction)\n",
    "p['Forecast Date'] = p['Forecast Date'].astype(str)\n",
    "\n",
    "res1 = evalOneSubmissionExtended.evalOneSub(truth.copy(), p.copy())\n",
    "res2 = simpleEvalOneSubmissionExtended.evalOneSub(truth.copy(), p.copy())\n",
    "                           \n",
    "assert outcomes_equal(res1, res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (i1, i2) in enumerate(zip(res1, res2)):\n",
    "    print(i, i1==i2, type(i1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.nan == np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation code expects as input:\n",
    "\n",
    "* `d4Df`: Pandas data frame containing the D4 dataset\n",
    "  * Required fields in the DataFrame:\n",
    "    * \n",
    "* `subDf`: Pandas data frame containing user forecasts for D2 subjects.\n",
    "  * Required fields in the DataFrame:\n",
    "    * Diagnosis\n",
    "    * ADAS13\n",
    "    * Ventricles_ICV\n",
    "    * Forecast Date (string in the format `%Y-%m`)\n",
    "    * RID: Patient ID\n",
    "    * CN relative probability\n",
    "    * MCI relative probability\n",
    "    * AD relative probability\n",
    "    * ADAS13 50% CI lower\n",
    "    * ADAS13 50% CI upper\n",
    "    * Ventricles_ICV 50% CI lower\n",
    "    * Ventricles_ICV 50% CI upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "a = datetime.datetime.now()\n",
    "b = a.strftime('%Y-%m')\n",
    "type(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.datetime.strptime(b, '%Y-%m')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_time_between_visits(df):\n",
    "    df1 = df[['RID', 'EXAMDATE']].copy()\n",
    "    df1 = df1.sort_values(by=['RID', 'EXAMDATE'])\n",
    "    df1['EXAMDATE'] = df1['EXAMDATE'].map(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d'))\n",
    "    # Get future value from each row's next row\n",
    "    df1[\"Future_EXAMDATE\"] = df1['EXAMDATE'].shift(-1)\n",
    "    # Drop each last row per patient\n",
    "    df1 = df1.drop(df1.groupby('RID').tail(1).index.values)\n",
    "    return (df1['Future_EXAMDATE']-df1['EXAMDATE']).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_time_between_visits(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
